import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
from datetime import datetime, date
import seaborn as sns
from collections import Counter
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê¸°ê°„ë³„ ì›Œë“œí´ë¼ìš°ë“œ ë¶„ì„ê¸°",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['Malgun Gothic', 'DejaVu Sans']

class StreamlitWordCloudAnalyzer:
    def __init__(self):
        # í™•ì¥ëœ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
        self.korean_stopwords = {
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ë“¤', 'ëŠ”', 'ì€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ê°€', 'ê³¼', 'ì™€', 'ë„', 'ë§Œ',
            'ì´ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'í•˜ë‹¤', 'ê°™ë‹¤', 'ë³´ë‹¤', 'ë”', 'ë§¤ìš°', 'ì •ë§', 'ì•„ì£¼', 'ì¡°ê¸ˆ', 'ì¢€',
            'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´', 'ê´€í•´', 'ë”°ë¼', 'ìœ„í•œ', 'í†µí•œ', 'ëŒ€í•œ', 'ê´€í•œ', 'ë”°ë¥¸',
            'ë“±', 'ë°', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ì¦‰', 'ì˜ˆë¥¼',
            'ìˆ˜', 'ê°œ', 'ëª…', 'ê±´', 'ì ', 'ë²ˆ', 'ì°¨', 'íšŒ', 'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ',
            'ëª¨ë“ ', 'ê°', 'ì–´ë–¤', 'ì—¬ëŸ¬', 'ë‹¤ë¥¸', 'ìƒˆë¡œìš´', 'ì£¼ìš”', 'ì¤‘ìš”', 'íŠ¹ë³„', 'ì¼ë°˜',
            'ë°©ë²•', 'ì‹œìŠ¤í…œ', 'ì¥ì¹˜', 'ê¸°ìˆ ', 'ì—°êµ¬', 'ë¶„ì„', 'ê²°ê³¼', 'íš¨ê³¼', 'ì„±ëŠ¥', 'íŠ¹ì„±','í¬í•¨í•˜ëŠ”','ì¹˜í™˜ëœ'
        }
        
        # í™•ì¥ëœ ì˜ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ìƒë¬¼í•™/ì˜í•™ ìš©ì–´ ì¶”ê°€)
        self.english_stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'shall',
            'method', 'system', 'apparatus', 'device', 'present', 'invention', 'embodiment', 'according',
            'comprising', 'including', 'wherein', 'therefor', 'thereof', 'herein', 'said', 'such',
            'provided', 'configured', 'adapted', 'disposed', 'formed', 'made', 'using', 'based',
            'this', 'that', 'these', 'those', 'which', 'who', 'whom', 'whose',
            'it', 'its', 'they', 'their', 'them', 'he', 'his', 'she', 'her', 'we', 'our', 'us',
            'i', 'me', 'my', 'you', 'your', 'yours', 'he', 'him', 'she', 'her', 'they', 'them',
            'there', 'here', 'where', 'when', 'why', 'how', 'one', 'two', 'three', 'four', 'five',
            'first', 'second', 'third', 'next', 'last', 'many','sub','cell','cells','human','humans',
            'patient', 'patients', 'study', 'studies', 'analysis', 'results', 'data', 'showed',
            'significant', 'observed', 'found', 'increased', 'decreased', 'compared', 'control',
            'treatment', 'group', 'groups', 'effect', 'effects', 'level', 'levels', 'expression','within',
            # ìƒë¬¼í•™/ì˜í•™ ê´€ë ¨ ë¶ˆìš©ì–´ ì¶”ê°€
            'cell', 'cells', 'human', 'humans', 'patient', 'patients', 'study', 'studies',
            'analysis', 'results', 'data', 'showed', 'significant', 'observed', 'found',
            'increased', 'decreased', 'compared', 'control', 'treatment', 'group', 'groups',
            'effect', 'effects', 'level', 'levels', 'expression', 'protein', 'proteins',
            'gene', 'genes', 'response', 'activity', 'function', 'role', 'important',
            'potential', 'possible', 'previous', 'recent', 'current', 'novel', 'new',
            'research', 'investigation', 'examination', 'evaluation', 'assessment',
            'measured', 'determined', 'identified', 'demonstrated', 'revealed',
            'associated', 'related', 'induced', 'caused', 'performed', 'conducted',
            # ì¶”ê°€ ë¶ˆìš©ì–´ (íŠ¹í—ˆ/ë…¼ë¬¸ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤)
            'one', 'first', 'second', 'third', 'more', 'sequence', 'alkylene', 'medium', 'tissue',
            'methods', 'culture', 'organoids', 'stem', 'bone', 'organoid', 'marrow', 'least',
            'mammalian', 'any', 'acid', 'comprises', 'culturing', 'compositions', 'crispr', 'target',
            'composition', 'producing', 'disease', 'intestinal', 'inhibitor', 'tracr', 'systems',
            'selected', 'region', 'liver', 'cancer', 'use', 'enzyme', 'matrix', 'embryoid', 'free',
            'model', 'neural', 'vascular', 'step', 'substituted', 'tumor', 'compounds', 'which',
            'ring', 'tissues', 'rna', 'drug', 'each', 'provides', 'organ', 'kinase', 'sequences',
            'nucleic', 'subject', 'independently', 'delivery', 'form', 'polynucleotide', 'vegf',
            'hours', 'derived', 'vitro', 'also', 'bodies', 'mature', 'complex', 'having',
            'combination', 'pluripotent', 'containing', 'cycloalkyl', 'produced', 'serum', 'lipid',
            'nano', 'two', 'optionally', 'surface', 'population', 'chamber', 'scf', 'mate', 'alkyl',
            'thereby', 'agent', 'dimensional', 'extracellular', 'spheroids', 'network', 'described',
            'growth', 'well', 'administering', 'claim', 'application', 'example', 'further',
            'various', 'particular', 'specific', 'multiple', 'several', 'different', 'certain',
            'example', 'preferred', 'suitable', 'effective', 'useful', 'known', 'shown'
        }
        

    @st.cache_data
    def load_data(_self, patent_file=None, paper_file=None):
        """ë°ì´í„° ë¡œë“œ (ìºì‹œ ì ìš©)"""
        patent_df = None
        paper_df = None
        
        if patent_file is not None:
            try:
                patent_df = pd.read_csv(patent_file, encoding='utf-8')
                st.success(f"âœ… íŠ¹í—ˆ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(patent_df):,}ê±´")
            except Exception as e:
                st.error(f"âŒ íŠ¹í—ˆ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        if paper_file is not None:
            try:
                paper_df = pd.read_csv(paper_file, encoding='utf-8')
                st.success(f"âœ… ë…¼ë¬¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(paper_df):,}ê±´")
            except Exception as e:
                st.error(f"âŒ ë…¼ë¬¸ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        return patent_df, paper_df
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text) or text == '':
            return []
        
        text = str(text).lower()
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        words = text.split()
        
        filtered_words = []
        for word in words:
            word = word.strip()
            if (len(word) > 2 and 
                word not in self.korean_stopwords and 
                word not in self.english_stopwords and
                not word.isdigit() and
                not re.match(r'^[a-z]{1,2}$', word)):  # 1-2ê¸€ì ì˜ì–´ ë‹¨ì–´ ì œì™¸
                filtered_words.append(word)
        
        return filtered_words
    
    def filter_by_date(self, df, date_column, start_date, end_date):
        """ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§"""
        try:
            df[date_column] = pd.to_datetime(df[date_column], errors='coerce')
            start_date = pd.to_datetime(start_date)
            end_date = pd.to_datetime(end_date)
            
            mask = (df[date_column] >= start_date) & (df[date_column] <= end_date)
            return df[mask]
        except Exception as e:
            st.error(f"ë‚ ì§œ í•„í„°ë§ ì˜¤ë¥˜: {e}")
            return df
    
    def extract_patent_text(self, df, start_date, end_date):
        """íŠ¹í—ˆ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if df is None or df.empty:
            return []
        
        df_filtered = self.filter_by_date(df, 'ì¶œì›ì¼', start_date, end_date)
        all_words = []
        
        text_columns = ['ë°œëª…ì˜ëª…ì¹­(êµ­ë¬¸)', 'ë°œëª…ì˜ëª…ì¹­(ì˜ë¬¸)', 'ìš”ì•½(êµ­ë¬¸)', 'ìš”ì•½(ì˜ë¬¸)', 
                       'ëŒ€í‘œì²­êµ¬í•­(êµ­ë¬¸)', 'ëŒ€í‘œì²­êµ¬í•­(ì˜ë¬¸)']
        
        for col in text_columns:
            if col in df_filtered.columns:
                for text in df_filtered[col].dropna():
                    all_words.extend(self.preprocess_text(text))
        
        return all_words
    
    def extract_paper_text(self, df, start_date, end_date):
        """ë…¼ë¬¸ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if df is None or df.empty:
            return []
        
        df_filtered = self.filter_by_date(df, 'ê²Œì‹œ ë‚ ì§œ', start_date, end_date)
        all_words = []
        
        text_columns = ['ì œëª©', 'ì´ˆë¡', 'í‚¤ì›Œë“œ']
        
        for col in text_columns:
            if col in df_filtered.columns:
                for text in df_filtered[col].dropna():
                    all_words.extend(self.preprocess_text(text))
        
        return all_words
    
    def generate_wordcloud_data(self, patent_df, paper_df, start_date, end_date, data_type='both', top_n=100):
        """ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„° ìƒì„±"""
        all_words = []
        
        if data_type in ['patent', 'both']:
            patent_words = self.extract_patent_text(patent_df, start_date, end_date)
            all_words.extend(patent_words)
        
        if data_type in ['paper', 'both']:
            paper_words = self.extract_paper_text(paper_df, start_date, end_date)
            all_words.extend(paper_words)
        
        word_freq = Counter(all_words)
        top_words = dict(word_freq.most_common(top_n))
        
        return top_words, len(all_words), len(word_freq)

# ë©”ì¸ ì•±
def main():
    st.title("ğŸ“Š ê¸°ê°„ë³„ ì›Œë“œí´ë¼ìš°ë“œ ë¶„ì„ê¸°")
    st.markdown("---")
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = StreamlitWordCloudAnalyzer()
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'patent_df' not in st.session_state:
        st.session_state.patent_df = None
    if 'paper_df' not in st.session_state:
        st.session_state.paper_df = None
    if 'data_loaded' not in st.session_state:
        st.session_state.data_loaded = False
    if 'search_history' not in st.session_state:
        st.session_state.search_history = []
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("ğŸ”§ ì„¤ì •")
    
    # ë°ì´í„° ìƒíƒœ í‘œì‹œ
    if st.session_state.data_loaded:
        st.sidebar.success("âœ… ë°ì´í„°ê°€ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        if st.session_state.patent_df is not None:
            st.sidebar.info(f"ğŸ“„ íŠ¹í—ˆ ë°ì´í„°: {len(st.session_state.patent_df):,}ê±´")
        if st.session_state.paper_df is not None:
            st.sidebar.info(f"ğŸ“ ë…¼ë¬¸ ë°ì´í„°: {len(st.session_state.paper_df):,}ê±´")
        
        # ë°ì´í„° ì´ˆê¸°í™” ë²„íŠ¼
        if st.sidebar.button("ğŸ”„ ë°ì´í„° ì´ˆê¸°í™”", type="secondary"):
            st.session_state.patent_df = None
            st.session_state.paper_df = None
            st.session_state.data_loaded = False
            st.session_state.search_history = []
            st.rerun()
    
    # íŒŒì¼ ì—…ë¡œë“œ (ë°ì´í„°ê°€ ì—†ì„ ë•Œë§Œ í‘œì‹œ)
    if not st.session_state.data_loaded:
        st.sidebar.subheader("ğŸ“ ë°ì´í„° ì—…ë¡œë“œ")
        patent_file = st.sidebar.file_uploader("íŠ¹í—ˆ ë°ì´í„° (CSV)", type=['csv'], key="patent")
        paper_file = st.sidebar.file_uploader("ë…¼ë¬¸ ë°ì´í„° (CSV)", type=['csv'], key="paper")
        
        if patent_file is not None or paper_file is not None:
            if st.sidebar.button("ğŸ“¥ ë°ì´í„° ë¡œë“œ", type="primary"):
                with st.spinner("ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘..."):
                    patent_df, paper_df = analyzer.load_data(patent_file, paper_file)
                    st.session_state.patent_df = patent_df
                    st.session_state.paper_df = paper_df
                    st.session_state.data_loaded = True
                    st.rerun()
    else:
        patent_df = st.session_state.patent_df
        paper_df = st.session_state.paper_df
    
    if not st.session_state.data_loaded:
        st.info("ğŸ‘† ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ë°ì´í„° ë¡œë“œ' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        st.stop()
    
    # ë°ì´í„° ë¡œë“œ ì™„ë£Œ í›„ì—ëŠ” ì„¸ì…˜ ìƒíƒœì—ì„œ ê°€ì ¸ì˜¤ê¸°
    patent_df = st.session_state.patent_df
    paper_df = st.session_state.paper_df
    
    # ë¶„ì„ ëª¨ë“œ ì„ íƒ
    st.sidebar.subheader("ğŸ¯ ë¶„ì„ ëª¨ë“œ")
    analysis_mode = st.sidebar.radio(
        "ë¶„ì„ ë°©ì‹ ì„ íƒ",
        ["ë‹¨ì¼ ë¶„ì„", "ë‹¤ì¤‘ ë¹„êµ ë¶„ì„"],
        help="ë‹¨ì¼: í•˜ë‚˜ì˜ ì¡°ê±´ìœ¼ë¡œ ë¶„ì„ / ë‹¤ì¤‘: ì—¬ëŸ¬ ì¡°ê±´ì„ í•œë²ˆì— ë¹„êµ"
    )
    
    if analysis_mode == "ë‹¨ì¼ ë¶„ì„":
        # ê¸°ì¡´ ë‹¨ì¼ ë¶„ì„ ì„¤ì •
        st.sidebar.subheader("ğŸ“Š ë¶„ì„ ì„¤ì •")
        data_type_options = []
        if patent_df is not None:
            data_type_options.append(("íŠ¹í—ˆë§Œ", "patent"))
        if paper_df is not None:
            data_type_options.append(("ë…¼ë¬¸ë§Œ", "paper"))
        if patent_df is not None and paper_df is not None:
            data_type_options.append(("ë‘˜ ë‹¤", "both"))
        
        data_type_label = st.sidebar.selectbox(
            "ë¶„ì„í•  ë°ì´í„°",
            [label for label, _ in data_type_options],
            index=len(data_type_options)-1 if len(data_type_options) > 1 else 0
        )
        data_type = next(value for label, value in data_type_options if label == data_type_label)
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        col1, col2 = st.sidebar.columns(2)
        with col1:
            start_date = st.date_input("ì‹œì‘ ë‚ ì§œ", date(2023, 1, 1))
        with col2:
            end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", date(2024, 12, 31))
        
        # í‚¤ì›Œë“œ ê°œìˆ˜ ì„¤ì •
        top_n = st.sidebar.slider("í‘œì‹œí•  í‚¤ì›Œë“œ ìˆ˜", 20, 200, 100, 10)
        
        # ë‹¨ì¼ ë¶„ì„ ì‹¤í–‰
        if st.sidebar.button("ğŸš€ ë¶„ì„ ì‹¤í–‰", type="primary"):
            # ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            search_config = {
                "data_type": data_type_label,
                "start_date": str(start_date),
                "end_date": str(end_date),
                "top_n": top_n,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # ì¤‘ë³µ ê²€ìƒ‰ ë°©ì§€ (ê°™ì€ ì„¤ì •ì´ë©´ ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
            if not any(h['data_type'] == search_config['data_type'] and 
                      h['start_date'] == search_config['start_date'] and 
                      h['end_date'] == search_config['end_date'] 
                      for h in st.session_state.search_history):
                st.session_state.search_history.append(search_config)
                # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ì €ì¥
                if len(st.session_state.search_history) > 10:
                    st.session_state.search_history.pop(0)
            
            # ë‹¨ì¼ ë¶„ì„ ì‹¤í–‰
            execute_single_analysis(analyzer, patent_df, paper_df, data_type, data_type_label, start_date, end_date, top_n)
    
    else:  # ë‹¤ì¤‘ ë¹„êµ ë¶„ì„
        st.sidebar.subheader("ğŸ“Š ë‹¤ì¤‘ ë¹„êµ ì„¤ì •")
        
        # ì„¸ì…˜ ìƒíƒœì— ë¹„êµ ì„¤ì • ì €ì¥
        if 'comparison_configs' not in st.session_state:
            st.session_state.comparison_configs = []
        
        # ìƒˆë¡œìš´ ë¹„êµ ì¡°ê±´ ì¶”ê°€
        with st.sidebar.expander("â• ìƒˆ ì¡°ê±´ ì¶”ê°€", expanded=True):
            # ë°ì´í„° ìœ í˜• ì„ íƒ
            data_type_options = []
            if patent_df is not None:
                data_type_options.append(("íŠ¹í—ˆë§Œ", "patent"))
            if paper_df is not None:
                data_type_options.append(("ë…¼ë¬¸ë§Œ", "paper"))
            if patent_df is not None and paper_df is not None:
                data_type_options.append(("ë‘˜ ë‹¤", "both"))
            
            new_data_type_label = st.selectbox(
                "ë°ì´í„° ìœ í˜•",
                [label for label, _ in data_type_options],
                key="new_data_type"
            )
            new_data_type = next(value for label, value in data_type_options if label == new_data_type_label)
            
            # ë‚ ì§œ ë²”ìœ„
            col1, col2 = st.columns(2)
            with col1:
                new_start_date = st.date_input("ì‹œì‘", date(2023, 1, 1), key="new_start")
            with col2:
                new_end_date = st.date_input("ì¢…ë£Œ", date(2024, 12, 31), key="new_end")
            
            # ì¡°ê±´ ì´ë¦„
            condition_name = st.text_input("ì¡°ê±´ ì´ë¦„", f"{new_data_type_label}({new_start_date}~{new_end_date})", key="new_name")
            
            # ì¶”ê°€ ë²„íŠ¼
            if st.button("â• ì¡°ê±´ ì¶”ê°€"):
                new_config = {
                    "name": condition_name,
                    "data_type": new_data_type,
                    "data_type_label": new_data_type_label,
                    "start_date": new_start_date,
                    "end_date": new_end_date
                }
                st.session_state.comparison_configs.append(new_config)
                st.rerun()
        
        # ì¶”ê°€ëœ ì¡°ê±´ë“¤ í‘œì‹œ
        if st.session_state.comparison_configs:
            st.sidebar.subheader("ğŸ“‹ ë¹„êµ ì¡°ê±´ ëª©ë¡")
            for i, config in enumerate(st.session_state.comparison_configs):
                col1, col2 = st.sidebar.columns([3, 1])
                with col1:
                    st.write(f"**{config['name']}**")
                    st.caption(f"{config['data_type_label']} | {config['start_date']} ~ {config['end_date']}")
                with col2:
                    if st.button("âŒ", key=f"remove_{i}"):
                        st.session_state.comparison_configs.pop(i)
                        st.rerun()
            
            # ì „ì²´ í‚¤ì›Œë“œ ìˆ˜ ì„¤ì •
            comparison_top_n = st.sidebar.slider("ê° ì¡°ê±´ë³„ í‚¤ì›Œë“œ ìˆ˜", 20, 100, 50, 10)
            
            # ë‹¤ì¤‘ ë¹„êµ ì‹¤í–‰
            if st.sidebar.button("ğŸ” ë‹¤ì¤‘ ë¹„êµ ì‹¤í–‰", type="primary"):
                execute_comparison_analysis(analyzer, patent_df, paper_df, st.session_state.comparison_configs, comparison_top_n)
            
            # ì¡°ê±´ ì „ì²´ ì‚­ì œ
            if st.sidebar.button("ğŸ—‘ï¸ ëª¨ë“  ì¡°ê±´ ì‚­ì œ"):
                st.session_state.comparison_configs = []
                st.rerun()
# ë‹¨ì¼ ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜
def execute_single_analysis(analyzer, patent_df, paper_df, data_type, data_type_label, start_date, end_date, top_n):
    with st.spinner("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
        # ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„° ìƒì„±
        word_freq, total_words, unique_words = analyzer.generate_wordcloud_data(
            patent_df, paper_df, start_date, end_date, data_type, top_n
        )
        
        if not word_freq:
            st.warning("âš ï¸ ì„ íƒí•œ ê¸°ê°„ê³¼ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()
        
        # í†µê³„ ì •ë³´ í‘œì‹œ
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ“ ì´ ë‹¨ì–´ ìˆ˜", f"{total_words:,}")
        with col2:
            st.metric("ğŸ”¤ ê³ ìœ  ë‹¨ì–´ ìˆ˜", f"{unique_words:,}")
        with col3:
            st.metric("â­ ìƒìœ„ í‚¤ì›Œë“œ", f"{len(word_freq)}")
        with col4:
            st.metric("ğŸ“Š ìµœê³  ë¹ˆë„", f"{max(word_freq.values()) if word_freq else 0}")
        
        st.markdown("---")
        
        # íƒ­ìœ¼ë¡œ ê²°ê³¼ í‘œì‹œ
        tab1, tab2, tab3 = st.tabs(["ğŸŒŸ ì›Œë“œí´ë¼ìš°ë“œ", "ğŸ“ˆ í‚¤ì›Œë“œ ìˆœìœ„", "ğŸ“‹ ìƒì„¸ ëª©ë¡"])
        
        with tab1:
            st.subheader("ğŸŒŸ ì›Œë“œí´ë¼ìš°ë“œ")
            
            # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
            fig, ax = plt.subplots(figsize=(15, 8))
            
            try:
                wordcloud = WordCloud(
                    width=1200, 
                    height=600,
                    background_color='white',
                    max_words=top_n,
                    colormap='viridis',
                    random_state=42,
                    font_path=None  # ì‹œìŠ¤í…œ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
                ).generate_from_frequencies(word_freq)
                
                ax.imshow(wordcloud, interpolation='bilinear')
                ax.axis('off')
                ax.set_title(f'ì›Œë“œí´ë¼ìš°ë“œ: {data_type_label} ({start_date} ~ {end_date})', 
                            fontsize=16, fontweight='bold', pad=20)
                
                st.pyplot(fig)
                
                # ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                import io
                img_buffer = io.BytesIO()
                fig.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight', 
                           facecolor='white', edgecolor='none')
                img_buffer.seek(0)
                
                st.download_button(
                    label="ğŸ“¥ ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (PNG)",
                    data=img_buffer.getvalue(),
                    file_name=f"wordcloud_{data_type}_{start_date}_{end_date}.png",
                    mime="image/png"
                )
                
            except Exception as e:
                st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
                st.info("ğŸ’¡ í•œê¸€ í°íŠ¸ê°€ ì—†ì–´ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        with tab2:
            st.subheader("ğŸ“ˆ ìƒìœ„ í‚¤ì›Œë“œ ìˆœìœ„")
            
            # ìƒìœ„ 30ê°œ í‚¤ì›Œë“œ ë§‰ëŒ€ ê·¸ë˜í”„
            top_30 = dict(list(word_freq.items())[:30])
            
            fig = go.Figure([go.Bar(
                x=list(top_30.values()),
                y=list(top_30.keys()),
                orientation='h',
                marker_color='lightblue',
                text=list(top_30.values()),
                textposition='auto',
            )])
            
            fig.update_layout(
                title=f"ìƒìœ„ 30ê°œ í‚¤ì›Œë“œ ë¹ˆë„ ({start_date} ~ {end_date})",
                xaxis_title="ë¹ˆë„",
                yaxis_title="í‚¤ì›Œë“œ",
                height=800,
                yaxis={'categoryorder':'total ascending'}
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with tab3:
            st.subheader("ğŸ“‹ í‚¤ì›Œë“œ ìƒì„¸ ëª©ë¡")
            
            # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í‘œì‹œ
            df_keywords = pd.DataFrame([
                {"ìˆœìœ„": i+1, "í‚¤ì›Œë“œ": word, "ë¹ˆë„": count, "ë¹„ìœ¨(%)": round(count/total_words*100, 2)}
                for i, (word, count) in enumerate(word_freq.items())
            ])
            
            # ê²€ìƒ‰ ê¸°ëŠ¥
            search_term = st.text_input("ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰")
            if search_term:
                df_filtered = df_keywords[df_keywords['í‚¤ì›Œë“œ'].str.contains(search_term, case=False, na=False)]
                st.dataframe(df_filtered, use_container_width=True)
            else:
                st.dataframe(df_keywords, use_container_width=True)
            
            # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            csv = df_keywords.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ í‚¤ì›Œë“œ ëª©ë¡ ë‹¤ìš´ë¡œë“œ (CSV)",
                data=csv,
                file_name=f"keywords_{data_type}_{start_date}_{end_date}.csv",
                mime="text/csv"
            )

# ë‹¤ì¤‘ ë¹„êµ ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜
def execute_comparison_analysis(analyzer, patent_df, paper_df, configs, top_n):
    if len(configs) < 2:
        st.warning("âš ï¸ ë¹„êµ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ ì¡°ê±´ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    with st.spinner("ë‹¤ì¤‘ ë¹„êµ ë¶„ì„ì„ ì‹¤í–‰í•˜ëŠ” ì¤‘..."):
        comparison_results = {}
        
        # ê° ì¡°ê±´ë³„ë¡œ ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„° ìƒì„±
        for config in configs:
            word_freq, total_words, unique_words = analyzer.generate_wordcloud_data(
                patent_df, paper_df, 
                config['start_date'], config['end_date'], 
                config['data_type'], top_n
            )
            
            comparison_results[config['name']] = {
                'word_freq': word_freq,
                'total_words': total_words,
                'unique_words': unique_words,
                'config': config
            }
        
        # ê²°ê³¼ í‘œì‹œ
        st.subheader("ğŸ” ë‹¤ì¤‘ ì¡°ê±´ ë¹„êµ ë¶„ì„ ê²°ê³¼")
        
        # ì „ì²´ í†µê³„ ë¹„êµ
        st.subheader("ğŸ“Š ì „ì²´ í†µê³„ ë¹„êµ")
        stats_data = []
        for name, result in comparison_results.items():
            stats_data.append({
                "ì¡°ê±´": name,
                "ì´ ë‹¨ì–´ ìˆ˜": result['total_words'],
                "ê³ ìœ  ë‹¨ì–´ ìˆ˜": result['unique_words'],
                "ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜": len(result['word_freq']),
                "ìµœê³  ë¹ˆë„": max(result['word_freq'].values()) if result['word_freq'] else 0
            })
        
        stats_df = pd.DataFrame(stats_data)
        st.dataframe(stats_df, use_container_width=True)
        
        # ì›Œë“œí´ë¼ìš°ë“œ ê·¸ë¦¬ë“œ í‘œì‹œ
        st.subheader("ğŸŒŸ ì›Œë“œí´ë¼ìš°ë“œ ë¹„êµ")
        
        # 2x2 ë˜ëŠ” 1xN ê·¸ë¦¬ë“œë¡œ í‘œì‹œ
        n_configs = len(configs)
        if n_configs <= 4:
            cols = 2
            rows = (n_configs + 1) // 2
        else:
            cols = 3
            rows = (n_configs + 2) // 3
        
        fig, axes = plt.subplots(rows, cols, figsize=(15, 6*rows))
        if rows == 1:
            axes = [axes] if cols == 1 else axes
        else:
            axes = axes.flatten()
        
        for i, (name, result) in enumerate(comparison_results.items()):
            if i >= len(axes):
                break
                
            try:
                if result['word_freq']:
                    wordcloud = WordCloud(
                        width=600, 
                        height=400,
                        background_color='white',
                        max_words=top_n//2,  # ê° ì›Œë“œí´ë¼ìš°ë“œëŠ” ì ˆë°˜ í¬ê¸°ë¡œ
                        colormap='viridis',
                        random_state=42,
                        font_path=None
                    ).generate_from_frequencies(result['word_freq'])
                    
                    axes[i].imshow(wordcloud, interpolation='bilinear')
                    axes[i].set_title(name, fontsize=12, fontweight='bold')
                    axes[i].axis('off')
                else:
                    axes[i].text(0.5, 0.5, 'ë°ì´í„° ì—†ìŒ', ha='center', va='center', fontsize=14)
                    axes[i].set_title(name, fontsize=12, fontweight='bold')
                    axes[i].axis('off')
            except Exception as e:
                axes[i].text(0.5, 0.5, f'ì˜¤ë¥˜: {str(e)}', ha='center', va='center', fontsize=10)
                axes[i].set_title(name, fontsize=12, fontweight='bold')
                axes[i].axis('off')
        
        # ë¹ˆ subplot ìˆ¨ê¸°ê¸°
        for i in range(len(comparison_results), len(axes)):
            axes[i].axis('off')
        
        plt.tight_layout()
        st.pyplot(fig)
        
        # í‚¤ì›Œë“œ êµì§‘í•© ë¶„ì„
        st.subheader("ğŸ”— ê³µí†µ í‚¤ì›Œë“œ ë¶„ì„")
        
        # ëª¨ë“  ì¡°ê±´ì˜ í‚¤ì›Œë“œ ìˆ˜ì§‘
        all_keywords = {}
        for name, result in comparison_results.items():
            for word, freq in result['word_freq'].items():
                if word not in all_keywords:
                    all_keywords[word] = {}
                all_keywords[word][name] = freq
        
        # ê³µí†µ í‚¤ì›Œë“œ ì°¾ê¸° (2ê°œ ì´ìƒ ì¡°ê±´ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œ)
        common_keywords = {word: data for word, data in all_keywords.items() 
                          if len(data) >= 2}
        
        if common_keywords:
            # ê³µí†µ í‚¤ì›Œë“œ í‘œ ìƒì„±
            common_df_data = []
            for word, conditions in common_keywords.items():
                row = {"í‚¤ì›Œë“œ": word}
                total_freq = 0
                for config in configs:
                    freq = conditions.get(config['name'], 0)
                    row[config['name']] = freq
                    total_freq += freq
                row["ì „ì²´ ë¹ˆë„"] = total_freq
                common_df_data.append(row)
            
            # ì „ì²´ ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬
            common_df = pd.DataFrame(common_df_data).sort_values("ì „ì²´ ë¹ˆë„", ascending=False)
            
            st.write(f"**2ê°œ ì´ìƒ ì¡°ê±´ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê³µí†µ í‚¤ì›Œë“œ: {len(common_keywords)}ê°œ**")
            st.dataframe(common_df.head(20), use_container_width=True)
            
            # ê³µí†µ í‚¤ì›Œë“œ CSV ë‹¤ìš´ë¡œë“œ
            csv_common = common_df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ ê³µí†µ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (CSV)",
                data=csv_common,
                file_name="common_keywords_analysis.csv",
                mime="text/csv"
            )
        else:
            st.info("ê³µí†µìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ì›Œë“œí´ë¼ìš°ë“œ ë¹„êµ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        import io
        img_buffer = io.BytesIO()
        fig.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight', 
                   facecolor='white', edgecolor='none')
        img_buffer.seek(0)
        
        st.download_button(
            label="ğŸ“¥ ë¹„êµ ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (PNG)",
            data=img_buffer.getvalue(),
            file_name="comparison_wordclouds.png",
            mime="image/png"
        )

    # ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ í‘œì‹œ
    if st.session_state.search_history:
        st.sidebar.markdown("---")
        st.sidebar.subheader("ğŸ“š ê²€ìƒ‰ íˆìŠ¤í† ë¦¬")
        
        for i, history in enumerate(reversed(st.session_state.search_history[-5:])):  # ìµœê·¼ 5ê°œë§Œ í‘œì‹œ
            with st.sidebar.expander(f"ğŸ” {history['data_type']} ({history['start_date']})"):
                st.write(f"**ë°ì´í„° ìœ í˜•:** {history['data_type']}")
                st.write(f"**ê¸°ê°„:** {history['start_date']} ~ {history['end_date']}")
                st.write(f"**í‚¤ì›Œë“œ ìˆ˜:** {history['top_n']}")
                st.write(f"**ê²€ìƒ‰ ì‹œê°„:** {history['timestamp']}")
                
                # ì¬ì‹¤í–‰ ë²„íŠ¼
                if st.button(f"ğŸ”„ ì¬ì‹¤í–‰", key=f"rerun_{i}"):
                    # ì„¤ì • ë³µì›
                    st.session_state.temp_data_type = history['data_type']
                    st.session_state.temp_start_date = history['start_date']
                    st.session_state.temp_end_date = history['end_date']
                    st.session_state.temp_top_n = history['top_n']
                    st.rerun()
        
        # íˆìŠ¤í† ë¦¬ ì „ì²´ ì‚­ì œ ë²„íŠ¼
        if st.sidebar.button("ğŸ—‘ï¸ íˆìŠ¤í† ë¦¬ ì‚­ì œ"):
            st.session_state.search_history = []
            st.rerun()
    
    # ì‚¬ìš©ë²• ì•ˆë‚´
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ“– ì‚¬ìš©ë²•")
    st.sidebar.markdown("""
    1. **ë°ì´í„° ì—…ë¡œë“œ**: íŠ¹í—ˆ ë˜ëŠ” ë…¼ë¬¸ CSV íŒŒì¼ ì—…ë¡œë“œ
    2. **ë°ì´í„° ë¡œë“œ**: 'ë°ì´í„° ë¡œë“œ' ë²„íŠ¼ í´ë¦­ (í•œ ë²ˆë§Œ!)
    3. **ë¶„ì„ ì„¤ì •**: ë°ì´í„° ìœ í˜•ê³¼ ê¸°ê°„ ì„ íƒ
    4. **ë¶„ì„ ì‹¤í–‰**: ë²„íŠ¼ì„ ëˆŒëŸ¬ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    5. **íˆìŠ¤í† ë¦¬**: ì´ì „ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¹ ë¥´ê²Œ ì¬ì‹¤í–‰
    """)
    
    # ì¶”ê°€ ì •ë³´
    with st.expander("â„¹ï¸ ë°ì´í„° í˜•ì‹ ì•ˆë‚´"):
        st.markdown("""
        **íŠ¹í—ˆ ë°ì´í„° ì»¬ëŸ¼:**
        - ì¶œì›ì¼, ë°œëª…ì˜ëª…ì¹­(êµ­ë¬¸/ì˜ë¬¸), ìš”ì•½(êµ­ë¬¸/ì˜ë¬¸), ëŒ€í‘œì²­êµ¬í•­(êµ­ë¬¸/ì˜ë¬¸)
        
        **ë…¼ë¬¸ ë°ì´í„° ì»¬ëŸ¼:**
        - ê²Œì‹œ ë‚ ì§œ, ì œëª©, ì´ˆë¡, í‚¤ì›Œë“œ
        
        **ë¶ˆìš©ì–´ ì²˜ë¦¬:**
        - í•œêµ­ì–´: ì¡°ì‚¬, ì–´ë¯¸, ì¼ë°˜ì ì¸ ë‹¨ì–´
        - ì˜ì–´: ê´€ì‚¬, ì „ì¹˜ì‚¬, ì¼ë°˜ì ì¸ ì—°êµ¬ ìš©ì–´ (cell, human, study ë“±)
        """)

if __name__ == "__main__":
    main()